{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mohnishdevadiga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mohnishdevadiga/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn import feature_extraction, model_selection, preprocessing\n",
    "from keras.layers import Activation, Dropout, Input, Embedding\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import glob \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from string import punctuation\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "for package in ['punkt','stopwords','wordnet','punkt']:\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/'+package)                \n",
    "    except LookupError:\n",
    "         nltk.download(package)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(shuffle=False,processed=False):\n",
    "    df = pd.DataFrame()\n",
    "    path = 'Datasets/*/*_*.csv'\n",
    "    if processed:\n",
    "        path = 'processed_dataset/*.csv'\n",
    "    for file in tqdm(glob.glob(path)):\n",
    "        df = df.append(pd.read_csv(file), ignore_index=True)\n",
    "    if shuffle:\n",
    "    \tdf = df.reindex(np.random.permutation(df.index)).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df,difference=300):\n",
    "    df_list = [df[df['fake']==0],df[df['fake']==1]] # [fact,fake]\n",
    "    lst = [len(df_list[0]),len(df_list[1])]\n",
    "    if not bool(lst.index(min(lst))):\n",
    "        \"\"\" if fake is larger then swap \"\"\"\n",
    "        df_list[0], df_list[1] = df_list[1], df_list[0]\n",
    "    size = len(df_list[0]) - len(df_list[1]) + difference\n",
    "    to_delete = random.sample(range(0, len(df_list[0])), size)\n",
    "    df_list[0] = df_list[0].drop(df_list[0].index[to_delete])\n",
    "    return df_list[0].append(df_list[1], ignore_index=True).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = \" \".join(x for x in word_tokenize(str(text)) if x.strip().lower() not in stop)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = \" \".join(lemmatizer.lemmatize(x.lower()) for x in text.split())\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(norm=True):\n",
    "    if norm:\n",
    "        df = normalize(get_datasets())\n",
    "        print(\"Normalized\")\n",
    "    else:\n",
    "        df = get_datasets(shuffle=True)\n",
    "    df = df.replace(np.nan, '', regex=True)\n",
    "    df['news'] = df['title'].str.cat(df['text'],sep=\" \")\n",
    "    print(\"Cleaning\")\n",
    "    df['news'] = df['news'].progress_apply(text_clean)\n",
    "    df = df.drop(['title','text'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(x, y=None, train=True, tokenizer=None,MAX_NB_WORDS=50000,MAX_SEQUENCE_LENGTH=300):\n",
    "    if not tokenizer:\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "        tokenizer.fit_on_texts(x)\n",
    "        print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "    x = tokenizer.texts_to_sequences(x)\n",
    "    x = pad_sequences(x, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    if not train:\n",
    "        return x, tokenizer\n",
    "    return x, y, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43601f3997234d32a5ddcd87971028eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>congressman met sanctioned putin friend moscow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>johnny cash manager holiff dy former manager j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>devos reveals chat fiercely critical teacher u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>keith schiller man trump sent fire comey washi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rush finish obama slap billion regulation amer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fake                                               news\n",
       "0     0  congressman met sanctioned putin friend moscow...\n",
       "1     0  johnny cash manager holiff dy former manager j...\n",
       "2     0  devos reveals chat fiercely critical teacher u...\n",
       "3     0  keith schiller man trump sent fire comey washi...\n",
       "4     1  rush finish obama slap billion regulation amer..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_datasets(shuffle=True,processed=True)\n",
    "df['news'] = df['news'].apply(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the size of Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4544c3f5c4064b2cb4f39dac3058c886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=209367.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "53291\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "threshold = 5\n",
    "\n",
    "def get_vocab(df, threshold):\n",
    "    global vocab\n",
    "    for i in tqdm(df['news'].values):\n",
    "        for key,val in {key: count for key, count in Counter(i.split()).items() if count >= threshold}.items():\n",
    "            vocab.add(key) \n",
    "\n",
    "get_vocab(df, threshold)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using GloVe to try and improve accuracy <a href='https://nlp.stanford.edu/projects/glove/'>Read More</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af9b49f83cd48c8b69f110eb15b0f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "def loadGloveModel(path):\n",
    "    print(\"Loading Glove Model\")\n",
    "    gloveModel = {}\n",
    "    for File in tqdm(glob.glob(path+'*')):\n",
    "        f = open(File,'r')\n",
    "        for line in f:\n",
    "            splitLines = line.split()\n",
    "            word = splitLines[0]\n",
    "            wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "            gloveModel[word] = wordEmbedding\n",
    "    print(len(gloveModel),\" words loaded!\")\n",
    "    return gloveModel\n",
    "\n",
    "embeddings_index = loadGloveModel('Glove/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into\n",
      "test: 146556, Counter({0: 121021, 1: 25535}) \n",
      "dev: 31405, Counter({0: 25890, 1: 5515})\n",
      "test: 31406, Counter({0: 25976, 1: 5430})\n"
     ]
    }
   ],
   "source": [
    "X, y = df['news'].values, df['fake'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2020)\n",
    "x_dev, x_test, y_dev, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=2020)\n",
    "print(\"Split data into\\ntest: {}, {} \\ndev: {}, {}\\ntest: {}, {}\".format(len(y_train),Counter(y_train),len(y_dev),Counter(y_dev),len(y_test),Counter(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 341289 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, tokenizer = format_data(x_train, y_train, MAX_NB_WORDS=len(vocab))\n",
    "x_dev, y_dev, tokenizer = format_data(x_dev, y_dev, MAX_NB_WORDS=len(vocab),tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "max_features = len(vocab)\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Trying 2 different models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential(inp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(vocab), 100, input_length=inp,weights=[embedding_matrix]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[300])\n",
    "    layer = Embedding(max_features,output_dim=embed_size,weights=[embedding_matrix],input_length=300)(inputs)\n",
    "    layer = LSTM(50)(layer)\n",
    "    layer = Dense(25,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=[inputs],outputs=[layer])\n",
    "    model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc8c603aff248e3851eb337ca139277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** 0 *****\n",
      "Train on 14656 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14656/14656 [==============================] - 286s 20ms/step - loss: 0.4294 - accuracy: 0.8294 - val_loss: 0.3630 - val_accuracy: 0.8615\n",
      "Epoch 2/10\n",
      "14656/14656 [==============================] - 310s 21ms/step - loss: 0.3289 - accuracy: 0.8781 - val_loss: 0.2766 - val_accuracy: 0.8960\n",
      "Epoch 3/10\n",
      "14656/14656 [==============================] - 284s 19ms/step - loss: 0.2660 - accuracy: 0.8992 - val_loss: 0.2256 - val_accuracy: 0.9178\n",
      "Epoch 4/10\n",
      "14656/14656 [==============================] - 283s 19ms/step - loss: 0.2286 - accuracy: 0.9147 - val_loss: 0.2099 - val_accuracy: 0.9268\n",
      "Epoch 5/10\n",
      "14656/14656 [==============================] - 241s 16ms/step - loss: 0.1981 - accuracy: 0.9286 - val_loss: 0.1874 - val_accuracy: 0.9349\n",
      "Epoch 6/10\n",
      "14656/14656 [==============================] - 245s 17ms/step - loss: 0.1795 - accuracy: 0.9347 - val_loss: 0.1839 - val_accuracy: 0.9388\n",
      "Epoch 7/10\n",
      "14656/14656 [==============================] - 225s 15ms/step - loss: 0.1649 - accuracy: 0.9416 - val_loss: 0.1646 - val_accuracy: 0.9437\n",
      "Epoch 8/10\n",
      "14656/14656 [==============================] - 204s 14ms/step - loss: 0.1506 - accuracy: 0.9445 - val_loss: 0.1658 - val_accuracy: 0.9445\n",
      "Epoch 9/10\n",
      "14656/14656 [==============================] - 203s 14ms/step - loss: 0.1301 - accuracy: 0.9548 - val_loss: 0.1579 - val_accuracy: 0.9482\n",
      "Epoch 10/10\n",
      "14656/14656 [==============================] - 200s 14ms/step - loss: 0.1177 - accuracy: 0.9595 - val_loss: 0.1725 - val_accuracy: 0.9481\n",
      "Epoch 00010: early stopping\n",
      "RNN\n",
      "Train on 14656 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14656/14656 [==============================] - 211s 14ms/step - loss: 0.1078 - accuracy: 0.9631 - val_loss: 0.1527 - val_accuracy: 0.9507\n",
      "Epoch 2/10\n",
      "14656/14656 [==============================] - 10887s 743ms/step - loss: 0.0929 - accuracy: 0.9679 - val_loss: 0.1581 - val_accuracy: 0.9526\n",
      "Epoch 3/10\n",
      "14656/14656 [==============================] - 27485s 2s/step - loss: 0.0874 - accuracy: 0.9688 - val_loss: 0.1456 - val_accuracy: 0.9555\n",
      "Epoch 4/10\n",
      "14656/14656 [==============================] - 182s 12ms/step - loss: 0.0779 - accuracy: 0.9725 - val_loss: 0.1493 - val_accuracy: 0.9550\n",
      "Epoch 00004: early stopping\n",
      "***** 1 *****\n",
      "Train on 14656 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14656/14656 [==============================] - 178s 12ms/step - loss: 0.1674 - accuracy: 0.9413 - val_loss: 0.1219 - val_accuracy: 0.9567\n",
      "Epoch 2/10\n",
      "14656/14656 [==============================] - 174s 12ms/step - loss: 0.1371 - accuracy: 0.9522 - val_loss: 0.1150 - val_accuracy: 0.9603\n",
      "Epoch 3/10\n",
      "14656/14656 [==============================] - 175s 12ms/step - loss: 0.1168 - accuracy: 0.9608 - val_loss: 0.1077 - val_accuracy: 0.9627\n",
      "Epoch 4/10\n",
      "14656/14656 [==============================] - 174s 12ms/step - loss: 0.1008 - accuracy: 0.9649 - val_loss: 0.1018 - val_accuracy: 0.9637\n",
      "Epoch 5/10\n",
      "14656/14656 [==============================] - 178s 12ms/step - loss: 0.0867 - accuracy: 0.9694 - val_loss: 0.1039 - val_accuracy: 0.9659\n",
      "Epoch 6/10\n",
      "14656/14656 [==============================] - 181s 12ms/step - loss: 0.0800 - accuracy: 0.9714 - val_loss: 0.1015 - val_accuracy: 0.9668\n",
      "Epoch 7/10\n",
      "14656/14656 [==============================] - 288s 20ms/step - loss: 0.0693 - accuracy: 0.9765 - val_loss: 0.1005 - val_accuracy: 0.9676\n",
      "Epoch 8/10\n",
      "14656/14656 [==============================] - 1903s 130ms/step - loss: 0.0574 - accuracy: 0.9808 - val_loss: 0.0980 - val_accuracy: 0.9692\n",
      "Epoch 9/10\n",
      "14656/14656 [==============================] - 191s 13ms/step - loss: 0.0499 - accuracy: 0.9831 - val_loss: 0.1044 - val_accuracy: 0.9687\n",
      "Epoch 00009: early stopping\n",
      "RNN\n",
      "Train on 14656 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14656/14656 [==============================] - 187s 13ms/step - loss: 0.0429 - accuracy: 0.9861 - val_loss: 0.0975 - val_accuracy: 0.9697\n",
      "Epoch 2/10\n",
      "14656/14656 [==============================] - 190s 13ms/step - loss: 0.0347 - accuracy: 0.9883 - val_loss: 0.1000 - val_accuracy: 0.9712\n",
      "Epoch 3/10\n",
      "14656/14656 [==============================] - 195s 13ms/step - loss: 0.0302 - accuracy: 0.9899 - val_loss: 0.0998 - val_accuracy: 0.9710\n",
      "Epoch 00003: early stopping\n",
      "***** 2 *****\n",
      "Train on 14656 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14656/14656 [==============================] - 220s 15ms/step - loss: 0.1037 - accuracy: 0.9648 - val_loss: 0.0816 - val_accuracy: 0.9727\n",
      "Epoch 2/10\n",
      "14656/14656 [==============================] - 210s 14ms/step - loss: 0.0771 - accuracy: 0.9720 - val_loss: 0.0777 - val_accuracy: 0.9743\n",
      "Epoch 3/10\n",
      "14656/14656 [==============================] - 229s 16ms/step - loss: 0.0641 - accuracy: 0.9781 - val_loss: 0.0926 - val_accuracy: 0.9708\n",
      "Epoch 00003: early stopping\n",
      "RNN\n",
      "Train on 14656 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14656/14656 [==============================] - 196s 13ms/step - loss: 0.0540 - accuracy: 0.9821 - val_loss: 0.0821 - val_accuracy: 0.9755\n",
      "Epoch 2/10\n",
      "14656/14656 [==============================] - 212s 14ms/step - loss: 0.0483 - accuracy: 0.9832 - val_loss: 0.0826 - val_accuracy: 0.9744\n",
      "Epoch 00002: early stopping\n",
      "***** 3 *****\n",
      "Train on 14656 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14656/14656 [==============================] - 216s 15ms/step - loss: 0.0906 - accuracy: 0.9691 - val_loss: 0.0750 - val_accuracy: 0.9744\n",
      "Epoch 2/10\n",
      "14656/14656 [==============================] - 228s 16ms/step - loss: 0.0724 - accuracy: 0.9742 - val_loss: 0.0715 - val_accuracy: 0.9775\n",
      "Epoch 3/10\n",
      "14656/14656 [==============================] - 231s 16ms/step - loss: 0.0602 - accuracy: 0.9801 - val_loss: 0.0676 - val_accuracy: 0.9783\n",
      "Epoch 4/10\n",
      "14656/14656 [==============================] - 249s 17ms/step - loss: 0.0501 - accuracy: 0.9828 - val_loss: 0.0692 - val_accuracy: 0.9785\n",
      "Epoch 5/10\n",
      "14656/14656 [==============================] - 234s 16ms/step - loss: 0.0440 - accuracy: 0.9847 - val_loss: 0.0705 - val_accuracy: 0.9785\n",
      "Epoch 00005: early stopping\n",
      "RNN\n",
      "Train on 14656 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14656/14656 [==============================] - 221s 15ms/step - loss: 0.0338 - accuracy: 0.9883 - val_loss: 0.0700 - val_accuracy: 0.9794\n",
      "Epoch 2/10\n",
      "14656/14656 [==============================] - 201s 14ms/step - loss: 0.0291 - accuracy: 0.9901 - val_loss: 0.0702 - val_accuracy: 0.9789\n",
      "Epoch 00002: early stopping\n",
      "***** 4 *****\n",
      "Train on 14656 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14656/14656 [==============================] - 217s 15ms/step - loss: 0.0824 - accuracy: 0.9726 - val_loss: 0.0654 - val_accuracy: 0.9787\n",
      "Epoch 2/10\n",
      "14656/14656 [==============================] - 232s 16ms/step - loss: 0.0636 - accuracy: 0.9801 - val_loss: 0.0618 - val_accuracy: 0.9801\n",
      "Epoch 3/10\n",
      "14656/14656 [==============================] - 206s 14ms/step - loss: 0.0552 - accuracy: 0.9810 - val_loss: 0.0619 - val_accuracy: 0.9801\n",
      "Epoch 00003: early stopping\n",
      "RNN\n",
      "Train on 14656 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14656/14656 [==============================] - 190s 13ms/step - loss: 0.0438 - accuracy: 0.9857 - val_loss: 0.0639 - val_accuracy: 0.9808\n",
      "Epoch 2/10\n",
      "14656/14656 [==============================] - 193s 13ms/step - loss: 0.0380 - accuracy: 0.9877 - val_loss: 0.0612 - val_accuracy: 0.9808\n",
      "Epoch 00002: early stopping\n",
      "***** 5 *****\n",
      "Train on 14656 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14656/14656 [==============================] - 189s 13ms/step - loss: 0.0700 - accuracy: 0.9765 - val_loss: 0.0630 - val_accuracy: 0.9796\n",
      "Epoch 2/10\n",
      "14656/14656 [==============================] - 190s 13ms/step - loss: 0.0583 - accuracy: 0.9796 - val_loss: 0.0589 - val_accuracy: 0.9807\n",
      "Epoch 3/10\n",
      "14656/14656 [==============================] - 707s 48ms/step - loss: 0.0467 - accuracy: 0.9853 - val_loss: 0.0586 - val_accuracy: 0.9817\n",
      "Epoch 4/10\n",
      "14656/14656 [==============================] - 200s 14ms/step - loss: 0.0387 - accuracy: 0.9870 - val_loss: 0.0634 - val_accuracy: 0.9807\n",
      "Epoch 00004: early stopping\n",
      "RNN\n",
      "Train on 14656 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14656/14656 [==============================] - 206s 14ms/step - loss: 0.0342 - accuracy: 0.9888 - val_loss: 0.0612 - val_accuracy: 0.9822\n",
      "Epoch 2/10\n",
      "14656/14656 [==============================] - 213s 15ms/step - loss: 0.0269 - accuracy: 0.9902 - val_loss: 0.0687 - val_accuracy: 0.9807\n",
      "Epoch 00002: early stopping\n",
      "***** 6 *****\n",
      "Train on 14655 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14655/14655 [==============================] - 225s 15ms/step - loss: 0.0696 - accuracy: 0.9761 - val_loss: 0.0649 - val_accuracy: 0.9777\n",
      "Epoch 2/10\n",
      "14655/14655 [==============================] - 211s 14ms/step - loss: 0.0555 - accuracy: 0.9812 - val_loss: 0.0542 - val_accuracy: 0.9824\n",
      "Epoch 3/10\n",
      "14655/14655 [==============================] - 214s 15ms/step - loss: 0.0459 - accuracy: 0.9838 - val_loss: 0.0541 - val_accuracy: 0.9832\n",
      "Epoch 4/10\n",
      "14655/14655 [==============================] - 187s 13ms/step - loss: 0.0379 - accuracy: 0.9882 - val_loss: 0.0596 - val_accuracy: 0.9824\n",
      "Epoch 00004: early stopping\n",
      "RNN\n",
      "Train on 14655 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14655/14655 [==============================] - 208s 14ms/step - loss: 0.0308 - accuracy: 0.9899 - val_loss: 0.0537 - val_accuracy: 0.9832\n",
      "Epoch 2/10\n",
      "14655/14655 [==============================] - 193s 13ms/step - loss: 0.0291 - accuracy: 0.9908 - val_loss: 0.0520 - val_accuracy: 0.9830\n",
      "Epoch 00002: early stopping\n",
      "***** 7 *****\n",
      "Train on 14655 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14655/14655 [==============================] - 175s 12ms/step - loss: 0.0691 - accuracy: 0.9769 - val_loss: 0.0505 - val_accuracy: 0.9837\n",
      "Epoch 2/10\n",
      "14655/14655 [==============================] - 186s 13ms/step - loss: 0.0564 - accuracy: 0.9809 - val_loss: 0.0532 - val_accuracy: 0.9833\n",
      "Epoch 00002: early stopping\n",
      "RNN\n",
      "Train on 14655 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14655/14655 [==============================] - 188s 13ms/step - loss: 0.0452 - accuracy: 0.9852 - val_loss: 0.0506 - val_accuracy: 0.9839\n",
      "Epoch 2/10\n",
      "14655/14655 [==============================] - 188s 13ms/step - loss: 0.0374 - accuracy: 0.9876 - val_loss: 0.0491 - val_accuracy: 0.9842\n",
      "Epoch 3/10\n",
      "14655/14655 [==============================] - 327s 22ms/step - loss: 0.0320 - accuracy: 0.9893 - val_loss: 0.0496 - val_accuracy: 0.9843\n",
      "Epoch 4/10\n",
      "14655/14655 [==============================] - 218s 15ms/step - loss: 0.0295 - accuracy: 0.9892 - val_loss: 0.0522 - val_accuracy: 0.9842\n",
      "Epoch 00004: early stopping\n",
      "***** 8 *****\n",
      "Train on 14655 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14655/14655 [==============================] - 227s 16ms/step - loss: 0.0595 - accuracy: 0.9800 - val_loss: 0.0525 - val_accuracy: 0.9831\n",
      "Epoch 2/10\n",
      "14655/14655 [==============================] - 230s 16ms/step - loss: 0.0470 - accuracy: 0.9846 - val_loss: 0.0479 - val_accuracy: 0.9842\n",
      "Epoch 3/10\n",
      "14655/14655 [==============================] - 243s 17ms/step - loss: 0.0379 - accuracy: 0.9870 - val_loss: 0.0492 - val_accuracy: 0.9844\n",
      "Epoch 4/10\n",
      "14655/14655 [==============================] - 224s 15ms/step - loss: 0.0350 - accuracy: 0.9878 - val_loss: 0.0508 - val_accuracy: 0.9840\n",
      "Epoch 00004: early stopping\n",
      "RNN\n",
      "Train on 14655 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14655/14655 [==============================] - 220s 15ms/step - loss: 0.0277 - accuracy: 0.9907 - val_loss: 0.0489 - val_accuracy: 0.9850\n",
      "Epoch 2/10\n",
      "14655/14655 [==============================] - 212s 14ms/step - loss: 0.0201 - accuracy: 0.9930 - val_loss: 0.0544 - val_accuracy: 0.9847\n",
      "Epoch 00002: early stopping\n",
      "***** 9 *****\n",
      "Train on 14655 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14655/14655 [==============================] - 218s 15ms/step - loss: 0.0625 - accuracy: 0.9798 - val_loss: 0.0463 - val_accuracy: 0.9847\n",
      "Epoch 2/10\n",
      "14655/14655 [==============================] - 201s 14ms/step - loss: 0.0482 - accuracy: 0.9844 - val_loss: 0.0429 - val_accuracy: 0.9859\n",
      "Epoch 3/10\n",
      "14655/14655 [==============================] - 209s 14ms/step - loss: 0.0412 - accuracy: 0.9864 - val_loss: 0.0433 - val_accuracy: 0.9856\n",
      "Epoch 00003: early stopping\n",
      "RNN\n",
      "Train on 14655 samples, validate on 31405 samples\n",
      "Epoch 1/10\n",
      "14655/14655 [==============================] - 189s 13ms/step - loss: 0.0338 - accuracy: 0.9889 - val_loss: 0.0453 - val_accuracy: 0.9856\n",
      "Epoch 2/10\n",
      "14655/14655 [==============================] - 204s 14ms/step - loss: 0.0278 - accuracy: 0.9907 - val_loss: 0.0455 - val_accuracy: 0.9855\n",
      "Epoch 00002: early stopping\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "callback = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1)\n",
    "              \n",
    "epochs = 10\n",
    "batch_size = 500\n",
    "try:\n",
    "    model_1 = sequential(x_train.shape[1])\n",
    "    model_2 = RNN()\n",
    "    history_1, history_2 = [], []\n",
    "    batches = 10\n",
    "    i = 0\n",
    "    for batch_x, batch_y in tqdm(zip(np.array_split(x_train, batches),np.array_split(y_train, batches))):\n",
    "        print('*'*5,i,'*'*5)\n",
    "        h_1 = model_1.fit(batch_x, batch_y, epochs=epochs, batch_size=batch_size,validation_data=(x_dev, y_dev), callbacks=[callback])\n",
    "        history_1.append(h_1)\n",
    "        print(\"RNN\")\n",
    "        h_2 = model_1.fit(batch_x, batch_y, epochs=epochs, batch_size=batch_size,validation_data=(x_dev, y_dev), callbacks=[callback])\n",
    "        history_2.append(h_2)\n",
    "        i+=1\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trump japan stress unity north korea talk trad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>melania trump ’ ‘ america first ’ inaugural wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>house democrat frustrated trump ’ national-sec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>investigation chairman house russia probe step...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dem sen manchin ’ truly believe ’ trump ’ abil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news  fake\n",
       "0  trump japan stress unity north korea talk trad...     0\n",
       "1  melania trump ’ ‘ america first ’ inaugural wa...     0\n",
       "2  house democrat frustrated trump ’ national-sec...     0\n",
       "3  investigation chairman house russia probe step...     0\n",
       "4  dem sen manchin ’ truly believe ’ trump ’ abil...     0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'news': x_test,'fake':y_test}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trial, tokenizer = format_data(x_test, train=False, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score Model_1:  0.9857988919314781\n",
      "Accuracy Score Model_2:  0.8271031013182194\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score Model_1: \",accuracy_score(y_test,model_1.predict_classes(x_trial)))\n",
    "print(\"Accuracy Score Model_2: \",accuracy_score(y_test,[1 if x > 0.6 else 0 for x in model_2.predict(x_trial)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving and testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact,fake :  25976 5430\n"
     ]
    }
   ],
   "source": [
    "fact, fake = [], []\n",
    "for txt, target in zip(x_test,y_test):\n",
    "    (fact,fake)[target == 1].append(txt)\n",
    "print(\"fact,fake : \",len(fact),len(fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data/fact.txt\", 'w') as output:\n",
    "    for row in fact:\n",
    "        output.write(str(row) + '\\n')\n",
    "with open(\"test_data/fake.txt\", 'w') as output:\n",
    "    for row in fake:\n",
    "        output.write(str(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_data/tokenizer.pickle', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_data/tokenizer.pickle', 'rb') as file:\n",
    "    tk = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trial1 , tk = format_data(x_test,train=False,tokenizer=tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(x_trial != x_trial1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.save('model_data/model.h5')\n",
    "model_json = model_1.to_json()\n",
    "with open(\"model_data/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "def load_model():\n",
    "    with open('model_data/model.json', 'r') as json_file:\n",
    "        model = model_from_json(json_file.read())\n",
    "    model.load_weights(\"model_data/model.h5\")\n",
    "    model._make_predict_function()\n",
    "    return model\n",
    "\n",
    "model_trial = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score Model_1:  0.9857988919314781\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score Model_1: \",accuracy_score(y_test,model_trial.predict_classes(x_trial)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
