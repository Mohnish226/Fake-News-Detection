{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mohnishdevadiga/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mohnishdevadiga/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohnishdevadiga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn import feature_extraction, model_selection, preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import glob \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import random\n",
    "from string import punctuation\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_NB_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohnishdevadiga/anaconda3/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(shuffle=False,processed=False):\n",
    "    df = pd.DataFrame()\n",
    "    path = 'Datasets/*/*_*.csv'\n",
    "    if processed:\n",
    "        path = 'processed_dataset/*.csv'\n",
    "    for file in tqdm(glob.glob(path)):\n",
    "        df = df.append(pd.read_csv(file), ignore_index=True)\n",
    "    if shuffle:\n",
    "    \tdf = df.reindex(np.random.permutation(df.index)).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df,difference=300):\n",
    "    df_list = [df[df['fake']==0],df[df['fake']==1]] # [fact,fake]\n",
    "    lst = [len(df_list[0]),len(df_list[1])]\n",
    "    if not bool(lst.index(min(lst))):\n",
    "        \"\"\" if fake is larger then swap \"\"\"\n",
    "        df_list[0], df_list[1] = df_list[1], df_list[0]\n",
    "    size = len(df_list[0]) - len(df_list[1]) + difference\n",
    "    to_delete = random.sample(range(0, len(df_list[0])), size)\n",
    "    df_list[0] = df_list[0].drop(df_list[0].index[to_delete])\n",
    "    return df_list[0].append(df_list[1], ignore_index=True).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = \" \".join(x for x in word_tokenize(str(text)) if x.strip().lower() not in stop)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = \" \".join(lemmatizer.lemmatize(x.lower()) for x in text.split())\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(norm=True):\n",
    "    if norm:\n",
    "        df = normalize(get_datasets())\n",
    "        print(\"Normalized\")\n",
    "    else:\n",
    "        df = get_datasets(shuffle=True)\n",
    "    df = df.replace(np.nan, '', regex=True)\n",
    "    df['news'] = df['title'].str.cat(df['text'],sep=\" \")\n",
    "    print(\"Cleaning\")\n",
    "    df['news'] = df['news'].progress_apply(text_clean)\n",
    "    df = df.drop(['title','text'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(df, train=True, tokenizer=None):\n",
    "    x = df[\"news\"].values\n",
    "    if train:\n",
    "        y = df['fake'].values\n",
    "    if not tokenizer:\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\'', lower=True)\n",
    "        tokenizer.fit_on_texts(x)\n",
    "        print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "    x = tokenizer.texts_to_sequences(x)\n",
    "    x = pad_sequences(x, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    if not train:\n",
    "        return x, tokenizer\n",
    "    return x, y, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab7c30746d3496a9cb6dcf66a5fc170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b3f3afa6bc4c9d8b2c8c0660b02a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=209367.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fake</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>germ live decade distilled water kill human ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>u.s. tax avoidance clampdown potential headach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>nunes admits secretly wh right magically found...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>actor life -square-foot apartment six year ago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>geek think rotten tomato mean superhero movie ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fake                                               news\n",
       "0     0  germ live decade distilled water kill human ho...\n",
       "1     0  u.s. tax avoidance clampdown potential headach...\n",
       "2     1  nunes admits secretly wh right magically found...\n",
       "3     0  actor life -square-foot apartment six year ago...\n",
       "4     0  geek think rotten tomato mean superhero movie ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pre_process(norm=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the processed file into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "chunk_df = np.array_split(df, 30)\n",
    "for chunk in chunk_df:\n",
    "    chunk.to_csv('processed_dataset/{}.csv'.format(i),index=False)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 410973 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "X, y, tokenizer = format_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=10, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=None, n_estimators=10, n_jobs=1,\n",
       "              nthread=None, objective='reg:squarederror', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "d, n, l = 10, 50, 0.5\n",
    "xgb_clf = xgb.XGBClassifier(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "xgb_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and Classifier Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8402827530209677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.99      0.91     34764\n",
      "           1       0.74      0.09      0.16      7110\n",
      "\n",
      "    accuracy                           0.84     41874\n",
      "   macro avg       0.79      0.54      0.54     41874\n",
      "weighted avg       0.83      0.84      0.78     41874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, xgb_clf.predict(x_test)))\n",
    "print(classification_report(y_test, xgb_clf.predict(x_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
